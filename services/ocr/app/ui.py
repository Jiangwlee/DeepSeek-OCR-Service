from __future__ import annotations

import asyncio
from pathlib import Path
from typing import Optional, Tuple

import gradio as gr

from .ocr_service.config import get_settings
from .ocr_service.exceptions import OCRException
from .ocr_service.schemas import DocumentOptions, DocumentResult
from .ocr_service.service import OCROrchestrator


def build_gradio_interface(orchestrator: OCROrchestrator) -> gr.Blocks:
    settings = get_settings()

    # è·å–å¯ç”¨æ¨¡å‹å’Œ prompt é¢„è®¾
    available_models = settings.get_available_models()
    prompt_presets = settings.get_prompt_presets()

    # æ„å»ºé€‰é¡¹
    model_choices = [(m["name"], m["value"]) for m in available_models]
    prompt_choices = [("è‡ªå®šä¹‰ Prompt", "custom")] + [(p["name"], p["value"]) for p in prompt_presets]

    async def handle_upload(file, output_format, model_provider, prompt_preset, custom_prompt):
        if file is None:
            return "è¯·å…ˆä¸Šä¼ æ–‡ä»¶ã€‚", "", ""

        path = Path(file.name)
        data = await asyncio.to_thread(path.read_bytes)

        # ç¡®å®šä½¿ç”¨çš„ prompt
        final_prompt = None
        if prompt_preset == "custom":
            final_prompt = custom_prompt if custom_prompt.strip() else None
        else:
            final_prompt = prompt_preset if prompt_preset else None

        options = DocumentOptions(
            output_format=output_format or "markdown",
            prompt=final_prompt,
            provider=model_provider,
        )

        result = await orchestrator.process_bytes(
            filename=path.name,
            content_type=None,
            data=data,
            options=options,
        )

        meta_md, text = _format_result(result, model_provider)
        preview = f"æ–‡ä»¶ï¼š{path.name}ï¼ˆ{path.stat().st_size} bytesï¼‰"
        return preview, meta_md, text

    async def wrapper(fn, *args):
        try:
            return await fn(*args)
        except OCRException as exc:
            return f"OCR å¤±è´¥ï¼š{exc}", "", ""
        except Exception as exc:  # pragma: no cover
            return f"æœªçŸ¥é”™è¯¯ï¼š{exc}", "", ""

    with gr.Blocks(title="Multi-Model OCR Playground", fill_height=True) as demo:
        gr.Markdown(
            """# ğŸš€ Multi-Model OCR Playground
**æ”¯æŒå¤šæ¨¡å‹**: DeepSeek-OCRã€Qwen3-VLã€PaddleOCR
å·¦ä¾§ï¼šä¸Šä¼ ä¸é…ç½®ï¼›å³ä¾§ï¼šç»“æœé¢„è§ˆ"""
        )

        with gr.Row(equal_height=True):
            with gr.Column(scale=1, min_width=420):
                gr.Markdown("## ğŸ“ æ–‡ä»¶ä¸Šä¼ ")
                upload_file = gr.File(
                    label="æ–‡æ¡£ï¼ˆdoc/docx/pdf/å›¾ç‰‡ï¼‰",
                    file_types=[".pdf", ".png", ".jpg", ".jpeg", ".webp", ".doc", ".docx"],
                    interactive=True,
                    visible=True,
                )
                file_preview = gr.Markdown("æœªé€‰æ‹©æ–‡ä»¶")

                gr.Markdown("## âš™ï¸ è¯†åˆ«é…ç½®")

                # æ¨¡å‹é€‰æ‹©ä¸‹æ‹‰æ¡†
                model_dropdown = gr.Dropdown(
                    choices=model_choices,
                    label="ğŸ¤– é€‰æ‹©æ¨¡å‹",
                    value=model_choices[0][1] if model_choices else "deepseek",
                    interactive=True,
                )

                # æ˜¾ç¤ºå½“å‰é€‰æ‹©çš„æ¨¡å‹ä¿¡æ¯
                model_info = gr.Markdown(
                    f"**å½“å‰æ¨¡å‹**: {available_models[0]['name']}\n"
                    f"**ç«¯ç‚¹**: `{available_models[0]['endpoint']}`"
                )

                # è¾“å‡ºæ ¼å¼
                output_format = gr.Radio(
                    ["markdown", "plain_text"],
                    label="ğŸ“„ è¾“å‡ºæ ¼å¼",
                    value="markdown"
                )

                # Prompt é¢„è®¾ä¸‹æ‹‰æ¡†
                prompt_dropdown = gr.Dropdown(
                    choices=prompt_choices,
                    label="ğŸ’¬ Prompt é¢„è®¾",
                    value="custom",
                    interactive=True,
                )

                # è‡ªå®šä¹‰ Prompt è¾“å…¥æ¡†
                custom_prompt_box = gr.Textbox(
                    label="âœï¸ è‡ªå®šä¹‰ Prompt",
                    placeholder="é€‰æ‹©ä¸Šæ–¹é¢„è®¾æˆ–è¾“å…¥è‡ªå®šä¹‰ Prompt",
                    lines=3,
                    visible=True,
                )

                submit_button = gr.Button("ğŸ” å¼€å§‹ OCR", variant="primary", size="lg")

            with gr.Column(scale=1, min_width=420):
                gr.Markdown("## ğŸ“Š ç»“æœé¢„è§ˆ")
                meta_output = gr.Markdown(label="ç»“æœæ¦‚è§ˆ")
                text_output = gr.Textbox(label="è¯†åˆ«ç»“æœ", lines=20, max_lines=30)

        # æ–‡ä»¶é€‰æ‹©æ—¶æ›´æ–°é¢„è§ˆ
        upload_file.change(
            lambda f: f"ğŸ“„ æ–‡ä»¶ï¼š**{Path(f.name).name}** ({Path(f.name).stat().st_size:,} bytes)" if f else "æœªé€‰æ‹©æ–‡ä»¶",
            inputs=upload_file,
            outputs=file_preview,
        )

        # æ¨¡å‹é€‰æ‹©æ—¶æ›´æ–°æ¨¡å‹ä¿¡æ¯
        def update_model_info(selected_model):
            for model in available_models:
                if model["value"] == selected_model:
                    return (
                        f"**å½“å‰æ¨¡å‹**: {model['name']}\n"
                        f"**ç«¯ç‚¹**: `{model['endpoint']}`\n"
                        f"**æ¨¡å‹ID**: `{model['model']}`"
                    )
            return "æ¨¡å‹ä¿¡æ¯æœªæ‰¾åˆ°"

        model_dropdown.change(
            update_model_info,
            inputs=model_dropdown,
            outputs=model_info,
        )

        # Prompt é¢„è®¾é€‰æ‹©æ—¶æ›´æ–°è‡ªå®šä¹‰è¾“å…¥æ¡†
        def update_custom_prompt(preset_value):
            if preset_value == "custom":
                return gr.update(visible=True, placeholder="è¾“å…¥è‡ªå®šä¹‰ Prompt", value="")
            else:
                return gr.update(visible=True, placeholder=f"å½“å‰ä½¿ç”¨é¢„è®¾: {preset_value[:50]}...", value="")

        prompt_dropdown.change(
            update_custom_prompt,
            inputs=prompt_dropdown,
            outputs=custom_prompt_box,
        )

        async def on_submit(file, output_format, model_provider, prompt_preset, custom_prompt):
            return await wrapper(handle_upload, file, output_format, model_provider, prompt_preset, custom_prompt)

        submit_button.click(
            on_submit,
            inputs=[
                upload_file,
                output_format,
                model_dropdown,
                prompt_dropdown,
                custom_prompt_box,
            ],
            outputs=[file_preview, meta_output, text_output],
            concurrency_limit=2,
        )

    return demo


def _format_result(result: DocumentResult, model_provider: str) -> Tuple[str, str]:
    """Format the OCR result for display."""
    # è·å–æ¨¡å‹åç§°
    settings = get_settings()
    available_models = settings.get_available_models()
    model_name = model_provider
    for model in available_models:
        if model["value"] == model_provider:
            model_name = model["name"]
            break

    meta_lines = [
        f"âœ… **OCR å®Œæˆ**",
        f"ğŸ¤– **ä½¿ç”¨æ¨¡å‹**: {model_name}",
        f"ğŸ“ **æ–‡æ¡£ ID**: `{result.document_id}`",
        f"ğŸ“„ **é¡µæ•°**: {result.total_pages}",
        f"ğŸ“‹ **è¾“å‡ºæ ¼å¼**: {result.output_format}",
    ]
    if result.stored_bucket:
        meta_lines.append(f"ğŸ’¾ **å·²å­˜å‚¨**: `{result.stored_bucket}/{result.stored_object_name}`")

    meta_md = "\n\n".join(meta_lines)
    return meta_md, result.combined_text
